{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2515954-4b16-4596-9dc3-c58de43d6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import platform \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce198ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = 'qwen'\n",
    "stop_stream = False\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name=='Windows' else 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a022ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_pretrained函数根据指定的模型名称或路径，从Hugging Face Model Hub（模型存储库）中加载相应的预训练tokenizer。\n",
    "# 加载的tokenizer可以直接用于对文本进行预处理，以便输入到预训练模型中进行推理或微调。\n",
    "# 共有一个固定参数，已经无固定参数\n",
    "# 固定参数：pretrained_model_name_or_path  预训练模型的名称（huggface）或者本地存储的路径\n",
    "# **kwargs：变参：常用的有\n",
    "#           cache_dir：指定模型文件的缓存目录。\n",
    "#           revision：指定要加载的模型的特定版本（通常用于从模型存储库中指定一个特定的Git提交版本）。\n",
    "#           proxies：指定用于下载模型文件的代理服务器。\n",
    "#           use_fast：指定是否使用快速模式加载tokenizer，即是否使用快速但不一定准确的实现。        \n",
    "tokenizer=AutoTokenizer.from_pretrained(modelPath, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8b711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"  # 使用CUDA\n",
    "DEVICE_ID = \"0\"  # CUDA设备ID，如果未设置则为空\n",
    "CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE  # 组合CUDA设备信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3a7883-700a-47df-854d-8a246ab8d570",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qwen2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 加载预训练的语言模型\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# device_map=\"map\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# torch_dtype=torch.bfloat16 这是指定模型参数的数据类型。在这里，模型的参数将使用16位浮点数（bfloat16）来存储\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:461\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    459\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    462\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    463\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    464\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:998\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m--> 998\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:710\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    711\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[0;32m    712\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'qwen2'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载预训练的语言模型\n",
    "# device_map=\"map\"\n",
    "# torch_dtype=torch.bfloat16 这是指定模型参数的数据类型。在这里，模型的参数将使用16位浮点数（bfloat16）来存储\n",
    "model=AutoModelForCausalLM.from_pretrained(modelPath,device_map=\"auto\",torch_dtype=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a43e49d-80dd-47e8-9d6f-c2308ab92e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(prompt,history=[]):\n",
    "    prompt = prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"现在你要扮演一个神舟软件公司开发的代码机器人--AspCoder\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():  # 检查是否可用CUDA\n",
    "        with torch.cuda.device(CUDA_DEVICE):  # 指定CUDA设备\n",
    "            torch.cuda.empty_cache()  # 清空CUDA缓存\n",
    "            torch.cuda.ipc_collect()  # 收集CUDA内存碎片\n",
    "\n",
    "def obtain_answer(input_str):\n",
    "    # 将文本作为输入，并将其转换为模型可以理解的数字序列。\n",
    "    text = tokenizer.apply_chat_template(input_str, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # 执行GPU内存清理\n",
    "    torch_gc()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df358a83-9ee8-4a8c-8800-22c5ac4c7787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用aspcoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 用户： 你叫什么名字\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AspCoder:我叫通义千问，是阿里云开源的超大规模语言模型\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 用户： stop\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    history = []\n",
    "    # 引用全局变量stop_stream，方法内修改一个全局变量的值时必须使用global声明\n",
    "    global stop_stream\n",
    "    print(\"欢迎使用aspcoder\")\n",
    "    while True:\n",
    "        # input() 函数用于从用户处接收输入。当调用 input() 函数时，程序会暂停执行，等待用户输入一些内容，并按下回车键。然后，input() 函数会将用户输入的内容作为一个字符串返回给程序\n",
    "        query = input(\"\\n 用户：\")\n",
    "        # strip() 函数去除了字符串 text 两端的空格\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            history = []\n",
    "            os.system(clear_command)\n",
    "            print(\"欢迎使用aspcoder\")\n",
    "            continue\n",
    "        count = 0\n",
    "        in_put=build_input(query,history)\n",
    "        # print(\"in_put:\"+in_put)\n",
    "        out_put=obtain_answer(in_put)\n",
    "        # history.append({'role':'assistant','content':out_put})\n",
    "        #   print(\"history\"+len(history))\n",
    "        print(\"AspCoder:\"+out_put+\"\\n\\n\")\n",
    "\n",
    "# Python 文件直接被运行时（而不是作为模块被导入时），执行下面缩进的代码块。\n",
    "# 这种写法常用于将一个 Python 文件既作为可执行脚本运行，又可以作为模块被其他 Python 文件导入和调用。\n",
    "# 在这种写法下，可以将一些需要在脚本直接运行时执行的初始化逻辑或主要功能封装在 main() 函数中，\n",
    "# 并通过 if __name__ == \"__main__\": 条件来控制它们的执行。\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7260e0-ab8a-4417-8f21-16478a80ce25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
