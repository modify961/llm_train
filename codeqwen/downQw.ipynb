{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c2c2a2-38c6-4700-863b-8226e3de1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:03:21,027 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-04-29 17:03:21,029 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-04-29 17:03:21,146 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 4fe6e4ce5cfd37bad88507890f907c4d and a total number of 976 components indexed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53ac1d9-5fbb-41ff-88e5-1d5de9028b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备名称: NVIDIA GeForce RTX 4090 D\n",
      "设备能力: (8, 9)\n",
      "设备支持bfloat16: 是\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_index = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(device_index)\n",
    "    bf16_support = torch.cuda.is_bf16_supported()\n",
    "    device_capability=torch.cuda.get_device_capability(device_index)\n",
    "    print(f\"设备名称: {device_name}\")\n",
    "    print(f\"设备能力: {device_capability}\")\n",
    "    print(f\"设备支持bfloat16: {'是' if bf16_support else '否'}\")\n",
    "else:\n",
    "    print(\"CUDA不可用\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6415d1-d67d-45ee-8774-e8e6332c0b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/finetune\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed351c0-f9ac-4e83-b95f-b51deb0009ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 701/701 [00:00<00:00, 475kB/s]\n",
      "Downloading: 100%|██████████| 73.0/73.0 [00:00<00:00, 40.1kB/s]\n",
      "Downloading: 100%|██████████| 212/212 [00:00<00:00, 119kB/s]\n",
      "Downloading: 100%|██████████| 6.73k/6.73k [00:00<00:00, 3.00MB/s]\n",
      "Downloading: 100%|█████████▉| 3.63G/3.63G [00:23<00:00, 163MB/s]\n",
      "Downloading: 100%|█████████▉| 3.68G/3.68G [00:21<00:00, 180MB/s]\n",
      "Downloading: 100%|█████████▉| 3.68G/3.68G [00:29<00:00, 135MB/s] \n",
      "Downloading: 100%|█████████▉| 2.52G/2.52G [00:22<00:00, 122MB/s] \n",
      "Downloading: 100%|██████████| 31.0k/31.0k [00:00<00:00, 862kB/s]\n",
      "Downloading: 100%|██████████| 3.43k/3.43k [00:00<00:00, 1.99MB/s]\n",
      "Downloading: 100%|██████████| 4.26M/4.26M [00:00<00:00, 12.5MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.58MB/s]\n",
      "Downloading: 100%|██████████| 971/971 [00:00<00:00, 516kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download('Qwen/CodeQwen1.5-7B-Chat', cache_dir='/root/finetune/qwen', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254178cc-8ea7-4966-9e4e-514e292a3515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
