# 训练类型
## SFT:
    SFT通常指的是“软参数共享”（Soft Parameter Sharing）。这是一种在微调过程中用来控制参数更新的技术，目的是在保持模型原有知识的同时，允许一定程度的适应性调整。具体来说，软参数共享技术通常通过引入额外的约束或正则化手段，使得模型在学习新任务时，其参数变动保持在一定的限度内，从而避免过拟合并保留预训练时学到的知识。这种方法特别适合于那些在大量预训练数据上学到丰富知识的大型模型，如BERT或GPT系列。
# 微调参数说明

## "output_dir": "output/qwen1.5-7b-sft-lora",
    记录微调过程日志、checkout、以及微调后参数文件的路径
## "model_name_or_path": "modles/Qwen",
    需要微调的的模型
## "train_file_path":"data",
    微调语料文件的路径，
## "train_file_name": "aspcoder_train.csv",
    微调的文件名字，格式为  category,human,assistant。需要存放在train_file_path指定的路径下的csv的文件夹下。系统微调时会自动读取对应的csv文件，将其转换为jsonl文件。同时读取至内存
- category：类型，一般作为标记语料的类型。
- human：提问的语句
- assistant：机器回答的内容

## "template_name": "qwen",
    模型的名称，取值一般从模型文件的config.json文件里面的model_type的值。
## "train_mode": "lora",
    训练模式：目前支持 
- lora:一种微调方式
    帮助文档：https://zhuanlan.zhihu.com/p/623543497
- qlora: 概念文档:https://zhuanlan.zhihu.com/p/666234324
- full: 全训练
## "num_train_epochs": 1,
整个训练数据集上训练模型的完整遍历次数：在大模型训练中，num_train_epochs 是一个训练参数，指的是在整个训练数据集上训练模型的完整遍历次数。每次遍历被称为一个“epoch”。这个参数用于控制训练过程的长度，即模型看到训练数据的次数。

例如，如果 num_train_epochs 设置为 3，这意味着整个训练数据集将被用来训练模型三次。这个参数对模型的学习效果和训练时间都有重要影响。较少的训练周期可能导致模型欠拟合，而过多的训练周期则可能导致过拟合，特别是当训练数据不足以支持重复学习时。
## "per_device_train_batch_size": 1,
每个设备上的训练批次大小

## "gradient_accumulation_steps": 16,
梯度累积算法：https://zhuanlan.zhihu.com/p/650710443

## "learning_rate": 2e-4,
   是用来控制模型参数更新的速率或步长的。它直接影响到模型在学习过程中参数的变化幅度。主要作用如下：
- 调整步长：learning_rate 决定了每一步优化算法（如梯度下降）中参数更新的幅度。如果学习率过高，模型可能会在最优解附近震荡甚至发散，导致训练失败；如果学习率过低，训练过程会非常缓慢，模型可能陷入局部最小值，或者在有限的训练时间内无法收敛到一个好的解。
- 影响收敛性：适当的学习率可以帮助模型更快地收敛到最优解，而不恰当的学习率会延长训练过程或导致训练结果不稳定。
   
   在微调预训练的大模型（如BERT、GPT等）时，通常会使用较小的学习率。这是因为这些模型已经在大量数据上进行了预训练，已经较好地捕捉了一般性的特征和知识。微调阶段主要是为了使模型能够适应具体的下游任务，通常不需要大幅度调整模型参数，较小的学习率有助于细微调整模型参数，防止破坏模型原有的有用知识。

   调整策略
- 预设调整：根据任务的复杂性和数据量来预先设定一个固定的学习率。
- 动态调整：在训练过程中根据一定的策略动态调整学习率，如学习率衰减、周期性调整等。
  选择合适的学习率通常需要依赖实验和经验，可能还会结合超参数调优技术，如网格搜索或随机搜索，以确定最优的学习率值。
## "max_seq_length": 1024,
   max_seq_length 参数在微调预训练的大模型中非常关键，尤其是在处理自然语言处理（NLP）任务时。这个参数定义了模型输入的最大序列长度，即模型一次能够处理的最大单元数（通常是单词或者词元的数量）。
作用
- 限制输入大小：由于许多深度学习模型，特别是基于Transformer的模型如BERT或GPT，要求输入数据的大小固定，max_seq_length 确保所有输入数据都被裁剪或填充到相同的长度，以满足模型的输入要求。
- 影响计算资源：更长的序列意味着模型处理单个输入时所需的计算资源更多。增加max_seq_length可以增加模型的表达能力，因为它允许模型查看更长的上下文，但同时也会显著增加内存消耗和训练时间。
- 平衡性能与效率：在实际应用中，通常需要在模型性能和计算效率之间找到平衡。较长的max_seq_length可能提升模型对上下文的理解能力，但也可能导致因计算资源限制而无法实际使用。
  
  微调中的设置策略，在微调预训练模型时，max_seq_length 的设置通常依赖于几个因素：
- 任务的需求：不同的任务对上下文长度的需求不同。例如，某些任务（如文本分类）可能只需要看到文本的一部分就足以做出决策，而其他任务（如问答系统）则可能需要完整的上下文来理解问题和提供准确的答案。
- 数据集的特点：查看数据集中文本的平均长度和长度分布可以帮助设定一个合理的max_seq_length。如果数据集中的文本通常很短，那么不需要设置很大的最大序列长度。
- 计算资源：资源的限制也是决定最大序列长度的重要因素。拥有更多的计算资源（如高端GPU或TPU）允许使用更长的序列。

  通常，在实际应用中，max_seq_length 会被设置为一个能够覆盖绝大多数样本的长度。例如，在BERT模型中，max_seq_length 常见的设置是 128 或 256，这个长度足以涵盖多数任务的需求，同时也考虑到了计算效率。如果内存和计算资源允许，有时也会设置为更长，如 512。
## "logging_steps": 100,
    每隔多少步统计一次train loss。一般设置为100即可
## "save_steps": 100,
    每隔多少步保存一个模型。一般设置为100即可
## "save_total_limit": 1,
    output_dir目录中最多保存多少个checkpoint，超出则会将最旧的删除。
## "lr_scheduler_type": "constant_with_warmup",
    lr_scheduler_type 参数在深度学习训练中用来指定学习率调度器的类型。学习率调度器是用来调整训练过程中学习率的策略，通过改变学习率来优化训练过程和提高模型性能。
    学习率调度器主要有以下作用：

- 帮助模型更快收敛：通过逐渐减少学习率，帮助模型在训练后期稳定收敛。
- 防止过拟合：调整学习率可以减少模型在训练数据上的过度拟合。
- 逃离局部最小值：在训练初期使用较高的学习率可能帮助模型逃离局部最小值，随后逐渐降低学习率以细化到全局最小值。

学习率调度器有多种类型，每种类型根据特定的规则调整学习率：

- 固定学习率：学习率在整个训练过程中保持不变。
- 阶梯式衰减（Step Decay）：在预定的周期后学习率减少一定的比例。
- 指数衰减：学习率按照一个固定的指数比例衰减。
- 余弦退火（Cosine Annealing）：学习率随着训练周期呈余弦函数形式变化。
- 线性衰减：学习率从一个高值线性减少到一个低值。
- 周期性调整：学习率周期性地增加和减少，有助于找到更好的全局最优解。
- Warmup：在训练初期先逐渐增加学习率，通常用来预防模型在一开始训练时由于学习率过高而导致的不稳定。

微调预训练的大模型时，通常会选择一种合适的学习率调度策略，以确保模型能在新的数据上有效学习而不丢失预训练期间获得的知识。常见的策略包括：

- 使用较小的学习率：由于预训练模型已经有较好的权重初始化，因此通常使用比原始预训练时更小的学习率。
- Warmup策略：初始使用较小的学习率，并在几个epoch后增加到一个更高的学习率，之后再逐步降低。这有助于模型平稳过渡到新任务。
- 线性或指数衰减：训练过程中逐步降低学习率，帮助模型在新任务上更好地收敛。

## "warmup_steps": 100,
warmup_steps 配合Warmup 策略的学习率调度器使用。它 定义了在训练初期逐渐增加学习率的步骤数。在这些步骤中，学习率从一个较小的初始值开始，逐渐增加到设定的最大学习率。
主要作用包括：

- 防止训练初期的不稳定：在模型训练的初始阶段直接使用高学习率可能会导致模型权重的大幅波动，影响模型的稳定性和最终性能。渐进地增加学习率可以避免这种情况。
- 更好的利用预训练权重：预训练模型通常已经接近最优解，使用 Warmup 可以帮助模型在保持预训练获得的知识的同时，平滑过渡到新任务。
- 提高模型收敛速度：通过初始的低学习率避免大的权重更新，可以更快地使模型适应新的数据集，从而加快收敛速度。

微调预训练的大模型时，warmup_steps 的具体设置取决于多种因素，如任务的复杂性、数据集的大小以及预训练模型的特性。常见的设置策略包括：

- 固定比例：将 warmup_steps 设为总训练步数的一个小比例，例如5%或10%。这种方法简单且常用，能够适应多数任务。
- 经验设定：基于类似任务和模型的先前经验来设定步数。例如，如果在类似的任务上10,000步的 Warmup 表现良好，可以考虑将其应用于新任务。
- 调试和实验：在实际训练过程中，可能需要根据模型在验证集上的表现来调整 warmup_steps 的值。通过多次实验确定最佳的步数通常可以达到更好的性能。

## "lora_rank": 64,
    lora_rank 参数是与 LoRA（Low-Rank Adaptation）技术相关的一个重要参数。LoRA 主要用于对大型预训练语言模型进行微调时的参数效率改进。
    LoRA 技术概述
        LoRA 通过在模型的关键部分（如 Transformer 的自注意力和前馈网络）插入低秩矩阵，达到调节模型权重的目的，而不直接修改预训练模型的参数。这种方法可以显著减少需要训练的参数数量，使得微调过程更加高效和快速。
    lora_rank 的作用
        lora_rank 参数指定了插入的低秩矩阵的秩（rank）。这个参数直接影响了 LoRA 的表达能力和参数数量：

- 较低的 lora_rank：使用更少的参数，提升计算效率，适用于资源受限或对效率要求较高的场景。
- 较高的 lora_rank：增强模型的表达能力，可能提高模型在特定任务上的性能，但相应地增加了计算复杂度和训练时间。

微调预训练大模型的设置， lora_rank 的具体值通常依赖于以下因素：

- 任务复杂度：对于复杂的任务，可能需要较高的 lora_rank 来捕捉更复杂的模式和关系。
- 数据集大小：较大的数据集可能允许或需要更高的 lora_rank 来避免过拟合，利用更丰富的数据特性。
- 计算资源和效率考量：在资源受限的情况下，选择一个较低的 lora_rank 可以减少所需的计算资源和训练时间。
- 经验和实验：参考相关文献中的经验或通过实验确定适合特定任务的 lora_rank 值。

## "lora_alpha": 16,
lora_alpha ，用于调节 LoRA 插入的参数在模型中的作用强度。具体控制了 LoRA 插入的矩阵与原有模型参数结合时的缩放比例。这个参数的设置决定了 LoRA 调整的强度，即 LoRA 参数对原有模型参数的影响程度：

- 较低的 lora_alpha：意味着 LoRA 的影响较小，模型的行为更接近原始的预训练模型。这种设置通常用于任务与预训练任务相似度较高，或者希望保持较多预训练特性的情况。
- 较高的 lora_alpha：提供了更强的调整能力，允许模型更深入地适应特定的下游任务。适用于任务特殊性较强，或者预训练模型与下游任务差异较大的情况。

微调预训练大模型时的设置建议，可以考虑以下几个方面：

- 任务相关性：评估下游任务与预训练任务的相关性。如果任务差异大，可能需要更高的 lora_alpha 以增强模型的任务适应性。
- 模型的泛化与专业化需求：根据是否需要模型更专注于特定任务，调整 lora_alpha 的值。高值可能导致模型在特定任务上表现优异，但可能牺牲一些泛化能力。
- 实验调整：由于不同任务和数据集的特性可能大相径庭，合适的 lora_alpha 值往往需要通过实验来确定。通常，可以从一个较小的值开始，逐步增加，观察模型性能的变化。

## "lora_dropout": 0.05,
在大模型微调过程中，lora_dropout 参数用于在 LoRA（Low-Rank Adaptation）模块中引入 dropout，以防止过拟合并提高模型的泛化能力。Dropout 是一种常见的正则化技术，它通过在训练过程中随机丢弃（即设置为零）一部分网络的连接，来阻止模型对训练数据的过度拟合。
在使用 LoRA 策略进行模型微调时，lora_dropout 的具体作用如下：

- 减少过拟合：在 LoRA 扩展的参数中使用 dropout，可以有效减少模型对特定训练样本的依赖，从而提高对未见数据的预测能力。
- 增强模型鲁棒性：通过在训练过程中随机地移除一些网络连接，dropout 帮助模型学习到更加鲁棒的特征，这些特征不会对训练数据中的随机噪声过度敏感。

当使用 LoRA 进行大模型的微调时，设置 lora_dropout 需要考虑以下几点：

- 任务的复杂性：对于复杂的任务或数据集，可能需要较高的 dropout 率来帮助模型避免过拟合。简单任务或数据较少的情况可能需要较低的 dropout 率，以免损失过多的信息。
- 模型大小和数据量：较大的模型和较小的数据集更容易过拟合，因此可能需要更高的 dropout 率。相反，如果数据集很大，模型的过拟合风险较低，可以适当降低 dropout 率。
- 实验调整：和其他训练参数一样，lora_dropout 的最佳设置通常需要通过实验来确定。可以从一个理论上合理的值开始，然后根据验证集的性能进行调整。

通常，在微调大模型时，可以开始时设置一个适中的 lora_dropout 率（例如 0.1 或 0.2），然后根据模型在开发集上的表现进行调整。如果发现模型在训练集上表现很好，但在验证集上表现不佳，这可能是过拟合的信号，此时应考虑增加 lora_dropout 率。反之，如果模型在训练集上的表现不佳，可能需要减少 dropout 率以允许模型更充分地学习数据的特征。

## "gradient_checkpointing": true,
    gradient_checkpointing 是一种用于深度学习模型中的内存优化技术，特别用于训练大型神经网络模型时。它允许模型在保持训练性能的同时减少内存使用量。这项技术在微调预训练的大模型时尤其有用，因为这些模型往往有大量的参数，需要大量内存。
    gradient_checkpointing 的作用

- 减少内存使用：通过在训练过程中存储少量的网络状态（如激活），而非整个计算图的所有中间激活，gradient_checkpointing 减少了内存占用。这样做的代价是在反向传播时需要重新计算一些前向传播的步骤。
- 使得训练大模型变得可行：在资源有限的情况下，如只有访问到有限的GPU内存，gradient_checkpointing 使得训练大型模型变得可能，否则这些模型可能因内存不足而无法训练。
- 延长训练时间：尽管 gradient_checkpointing 减少了内存消耗，但它会增加模型的训练时间，因为一些计算需要在反向传播时重新执行。

在使用 gradient_checkpointing 微调预训练大模型时，可以考虑以下因素：

- 内存限制：如果可用的硬件内存较少，而模型非常大，启用 gradient_checkpointing 可以帮助实现模型的训练。在资源不是问题的情况下，可能不需要使用这项技术。
- 训练时间和资源的权衡：使用 gradient_checkpointing 会使训练速度变慢，因此需要根据项目的时间和资源限制来权衡是否使用。
- 批大小（Batch size）的调整：启用 gradient_checkpointing 可能会允许使用更大的批大小进行训练，这可以有助于模型收敛性和训练稳定性。
- 实验性调整：和其他训练参数一样，是否启用 gradient_checkpointing 及其设置通常需要通过实验来确定最佳配置。

"disable_tqdm": false,
"optim": "paged_adamw_32bit",
"seed": 42,
"fp16": true,
"report_to": "tensorboard",
"dataloader_num_workers": 0,
"save_strategy": "steps",
"weight_decay": 0,
"max_grad_norm": 0.3,
"remove_unused_columns": false